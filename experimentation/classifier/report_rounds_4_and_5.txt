Round 5
=======

Description
-----------

Regarding the statistical classification stage of IEPY, the goal of this
round of experiments is:
    - Obtain good configurations for SVM that can be efficiently used with big
    datasets.
    (the same goal as in the previous round, given the bad results for it)

To do that, configurations will:
    - Keep working with sparse matrices (sparse=True).
    - Try a new reduced 7 feature set:
        - bag_of_words_in_between
        - bag_of_pos_in_between
        - bag_of_wordpos_in_between
        - entity_order
        - entity_distance
        - other_entities_in_between
        - verbs_count_in_between
    - Explore a variety of configurations for SVM: The same as in rounds 3 and 4,
    but with different feature selection. Specifically:
        - 2 kernels: RBF and polynomial of degree 4.
        - 4 gamma values: [0.0, 1e-4, 1e-5, 1e-6].
        - 5 class weights: [(10, 1), (1, 0.1), (1, 1), (0.1, 1), (1, 10)].
        - 4 values for frequency feature selection: [1 (no selection), 2, 5 10].
    (2*4*5*4 = 160 configurations in total)

Results
-------

See
- `experimentation/classifier/Analysis_round5.ipynb` and
- `experimentation/classifier/Analysis_round5_part2.ipynb`.

For the "diagnosed" relation:
- This set of features has the best results so far.
- **RBF kernel** with **gamma 0.0** is giving the best results.
- Best class weights:
    - **false:1,true:1**: good F1 and good precision.
    - **false:10,true:1**: slightly worse F1, slightly best precision.
    - false:1:true:10: slightly best F1, much worse precision.
- Filtering frequency:
    - It looks like it is not as influential as the previous parameters.
    - No filtering at all (equivalently, filtering with frequency 1) seems to be
    the worst solution.
    - **5 and 10** seems to be the best frequency filters among [1, 2, 5, 10].

With the 'causes' relation, this set of features works more or less like the
features used in round 3.
The results are not good in general, but the 'causes' dataset is too small to be
trusted.

The conclusions from round 5 part 1 ("diagnosed" relation) are not debunked,
with some minor observations:
- Poly shows up in the acceptable precision results.
- Filtering frequency: 
    - The best precision results are achieved with 1 (no filtering) and 2.
    - 10 and 5 still give acceptable precisions.

Ideas for the next rounds of experiments:
- Try slightly different feature sets, adding and/or removing one feature at a
  time.
- Try choosing the parameter for frequency filter in terms of the size of the
  dataset.


Round 4
=======

Description
-----------

Regarding the statistical classification stage of IEPY, the goal of this
round of experiments is:
    - Obtain good configurations for SVM that can be efficiently used with big
    datasets.

To do that, configurations will:
    - Repeat the experiments done in round 3 but with sparse matrices
    (sparse=True) to support potentially big datasets.

Results
-------

See `experimentation/classifier/Analysis_round4.ipynb`.

Among the explored configurations, there are no acceptable strategies for the
"diagnosed" relationship.
The best F1 was of 51%, but with a low precision curve that reaches 64%.
For bootstrap we desire to have much better precision.

Probably the big number of features leads to sparsity and overfitting problems.
Ideas for next round of experiments:
- Try a reduced feature set.
- Try feature selection by frequency filtering.

Main Focus on experimentation is to see how to maximize the general Precision & Recall
figures with the less possible input from Human.
That's why that, together with measuring what was mentioned above, we are also checking
the machine accuracy and size of guesses as an indication of what part of the knowledge
built was automatically done, and how good it's.



-------- Round 5 ------------
Description:
    Hand created configurations.
    Goal: to see the 4 champions of round 4 a bit further, with the 10% of the questions answered.

Conclusions:
    - 2 of the 8 runs produced very decent behavior.
    - Surprisely, those 2 nice IEPY configurations worked very badly only changing the seed facts

Results Exploration:
    experimentation/loop/Analisis_round5.ipynb

Results Data:
    DB "db_loop_experiments_results" on "goedel", dataset 8

Relation:
    DIAGNOSED
        1335 Possible evidences on the reference, 208 of them positives (15.58%)

Consisted on 8 finished experiments, varying:
    the 4 champions of round 4, each one 2 times with different seed facts


-------- Round 4 ------------
Description:
    Based on what was concluded on round 2:
        - Attempt to see if same conclusions for CAUSES apply for this bigger relation
        - Play a bit with some other classifiers well ranked
        - Check if the new option "Drop guesses each round" was worth

Conclusions:

    1/3 of the experiments still failed, but considered good enough.

    - Human never answered more than 5% of the possible questions
        -> it's worth exploring up to 10%
    - In most of the reasonable experiments, drop-previous-guesses was turned on
        - Now, probably because of that, answering questions by uncertainty is prefered over by score
    - Small number of questions per round is better (5 or 3)
    - Original champion classifier is still performing good, although one with poly kernel also roughly worked
    - Decision_function still prefered, but there are prospectives with predit and predit_proba too
    - 4 champions selected for round 5 (see them on notebook)

Results Exploration:
    experimentation/loop/Analisis_round4.ipynb

Results Data:
    Analysis made with rounds 3 and 4 together
    DB "db_loop_experiments_results" on "goedel", datasets 3, 5 and 6

Classifiers:
    3 classifiers:
        - the same that we were using before
        - 2 new ones extracted from experimentation/classifier/config_round4.py
    For performance sake, in all cases, the set of features slighly reduced
    (hand made decision) discarding those that were creating bags of all segment info.

Relation:
    DIAGNOSED
        1335 Possible evidences on the reference, 208 of them positives (15.58%)

Consisted on 466 finished experiments, varying:
    Similar to round 3, with fixed hacks to new classifiers, and
    new option for dropping guesses from previous rounds each time.



-------- Round 3 ------------
Description:
    Based on what was concluded on round 2:
        - Attempt to see if same conclusions for CAUSES apply for this bigger relation
        - Play a bit with some other classifiers well ranked

Conclusions:

    2/3 of the experiments failed, so instead of doing analysis of this,
    fixes were planned and attempted on round 4.
    Basically, the 2 new classifiers were not working.


Results Data:
    DB "db_loop_experiments_results" on "goedel", dataset 3

Classifiers:
    3 classifiers:
        - the same that we were using before
        - 2 new ones extracted from experimentation/classifier/config_round4.py
    For performance sake, in all cases, the set of features slighly reduced
    (hand made decision) discarding those that were creating bags of all segment info.

Relation:
    DIAGNOSED
        1335 Possible evidences on the reference, 208 of them positives (15.58%)

Consisted on 290 experiments, only 98 finished, varying:
    Very similar to round 2, with following changes
    - only considered evidence threshold higher than fact one
    - considered new classifiers



-------- Round 2 ------------
Description:
    Same goal than round 1, with bug fixes

Conclusions:

    - With Human answering 10% of the corpus, several experiments had
      promising behaviour (between 25 and 33% recall,
      between 70% and 88% precision)
    - Beyond that 10%, the growing of the number of Human answers, machine learning was never improving
    - In general, in none case machine accuracy increased simultaneously with number of predictions,
      or at leat mantaining the number of predictions
    - In the selected champions (can be seen on the notebook)
        - decision_function is the prefered prediction method
        - evidence threshold is always a higher number than fact threshold
        - questions sorted by "score" was highly prefered
        - prediction scaling was on 50% of the winner times.


Results Exploration:
    experimentation/loop/Analisis_round2.ipynb

Results Data:
    DB "db_loop_experiments_results" on "goedel", dataset 2

Classifiers:
    Same than round 1
    Proposed as unique candidate the champion of the
    experimentation/classifier/config_round3.py

Relation:
    Same than round 1
    CAUSES
        281 Possible evidences on the reference, 58 of them positives (20.64%)

Consisted on 514 experiments, varying:
    Very similar to round 1, with following changes
    - number of answers per round (3, 5, 7, 10)
    - wider ranges for thresholds (fact and evidence)


-------- Round 1 ------------
Description:
    Just exploratory, to see in general terms the evolution of IEPY with small variations

Conclusions:
    Buggy experimentation.
    The experiment design has a couple of flaws:
        - bug in the way of storing what the machine guesses on each round
        - human ends up answering entire corpus

Results Data:
    DB "db_loop_experiments_results" on "goedel", dataset 0

Classifiers:
    Proposed as unique candidate the champion of the
    experimentation/classifier/config_round3.py

Relation:
    CAUSES
        281 Possible evidences on the reference, 58 of them positives (20.64%)

Consisted on 386 experiments, varying:
    - fixed number of rounds: 15
    - number of answers per round (5, 15, 25)
    - thresholds (fact and evidence)
    - prediction method (predit, predit_proba, and decision_function)
    - prediction scaling or not
    - criteria for picking questions (score or uncertainty)
